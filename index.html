<!doctype html>
<html lang="en">
  <head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156935549-3"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-156935549-3');
  </script>



    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Other -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.4.2/handlebars.min.js"></script>

    <title>Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</title>
  </head>
  <body>

    <!-- header -->
    <div class='jumbotron' style="background-color:#e6e9ec">
    <div class="container">
    <h1 class="text-center">Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</h1>
    <p class='text-center'><a href="https://scholar.google.com/citations?user=VVIAoY0AAAAJ&hl=en" target="_blank">Jonah Philion</a>, <a href="http://www.cs.toronto.edu/~fidler/" target="_blank">Sanja Fidler</a></p>
    <p class='text-center'>NVIDIA, Vector Institute, University of Toronto</p>
    <p class='text-center'>ECCV 2020</p>
    <p class='text-center'><img src='imgs/nusc.gif' class='img-fluid' style='height:250px; border-radius:15px; padding:5px'></p>
    <!-- <iframe src="https://drive.google.com/file/d/1XwqzDYfzXhky1WNuXTKy7i-jVXgp4hc_/preview" width="640" height="480" autoplay></iframe> -->
    <!-- <div class="embed-responsive embed-responsive-16by9" style='height:250px; width:444px; margin:auto;'>
      <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/oL5ISk6BnDE?autoplay=1" allowfullscreen></iframe>
    </div> -->
    </div>
    </div>

    <div class="container">

      <p>
        The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these represen- tations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that di- rectly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eye- view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’s- eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eye- view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar.
      </p>

    <hr/>

    <span class="border border-white">
    <h4 class="text-center">News</h4>
    <ul>
      <li>[TBA] code release</li>
      <li>[TBA] paper released on arxiv</li>
      <li>[July 2020] camera-ready submitted [<a href="imgs/paper.pdf" target="_blank">paper</a>]</li>
    </ul>
    </span>

    <hr/>

    <span class="border border-white">
      <h4 class="text-center">Paper</h4>
      <div class='row'>
        <div class='col'>
          <img src='imgs/icon.jpg' class='img-fluid float-right' style='height:180px; border: solid; border-radius:30px;'>
        </div>
        <div class='col'>
          <p class="card-text">Jonah Philion, Sanja Fidler</p>
          <p class="card-text">Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</p>
          <p class="card-text">ECCV, 2020. (poster) (to appear)</p>
          <p class="card-text">[<a href="imgs/paper.pdf" target="_blank">preprint</a>] [bibtex]</p>
        </div>
      </div>
    </span>

    <hr/>
    <h4 class='text-center'>ECCV 2020 1 minute video</h4>
    <div class="embed-responsive embed-responsive-16by9">
      <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/ypQQUG4nFJY" allowfullscreen></iframe>
    </div>
    <br>

    <hr/>
    <h4 class='text-center'>ECCV 2020 10 minute video</h4>
    <div class="embed-responsive embed-responsive-16by9">
      <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/oL5ISk6BnDE" allowfullscreen></iframe>
    </div>
    <br>

    <!-- <div class="embed-responsive embed-responsive-16by9">
      <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/ngEN2N6Rkos" allowfullscreen></iframe>
    </div> -->


    <!-- 
    <hr/>

    <h4 class='text-center'>Synopsis of Results</h4>

    <b>Human evaluation</b> We generate pairs of detection mistakes such that, for every pair, NDS ranks one set of detection mistakes worse but PKL considers the other set of detection mistakes worse. We then ask humans to choose which mistake was more dangerous. We find that in 79% of the generated scenarios, the humans side with our metric, PKL.
    <div class='row justify-content-center'>
      <div class='col-auto'>
        <table class="table table-responsive text-center">
          <thead>
            <tr>
              <td>NDS</td>
              <td>PKL</td>
              <td>Scenes</td>
              <td>Responses</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th scope='row'><font color="red">21%</font></th>
              <th scope='row'><font color='green'>79%</font></th>
              <td>75</td>
              <td>730</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <p><b>Comparison to hand-designed metrics</b> On the left, we show that PKL largely agrees with NDS under canonical noise models such as translation error. On the right, we show that while NDS penalizes false negatives equally independent of context, PKL penalizes false negatives more strongly if the missed detection is close to the ego vehicle or moving at high speeds.</p>

    <div class='row'>
      <div class='col'>
        <img src='together.png' class='img-fluid'>
      </div>
    </div>

    <br>
    <p><b>False Negative/False Positive Sensitivity</b> We visualize the "importance" that each true object in the scene is detected correctly by removing each object from the scene and re-evaluating the PKL (left). The ego vehicle is shown in green. To visualize the sensitivity of PKL to false positives, we place false positives at each position on a grid local to the ego vehicle and evaluate the PKL (right). Qualitatively, the "worst" false negatives involve objects that are close to the ego vehicle and the "worst" false positives occur in regions where the  ego vehicle is likely to travel in the future.</p>

    
      <div class='row'>
        <div class='col-6 text-center'>
          <img src='importance.gif' class='img-fluid' style="border:1px solid #000000; border-radius: 25px;">
        </div>
        <div class='col-6 text-center'>
          <img src='fpos.gif' class='img-fluid' style="border:1px solid #000000; border-radius: 25px;">
        </div>
      </div>
    
    <hr/>
    </div>

    <h4 class="text-center">Supplementary</h4>
    <nav>
      <div class="nav nav-pills justify-content-center" id="nav-tab" role="tablist">
        <a class="nav-item nav-link active" id="nav-profile-tab" data-toggle="tab" href="#nav-profile" role="tab" aria-controls="nav-profile" aria-selected="false">Planner Visualization</a>
        <a class="nav-item nav-link" id="nav-home-tab" data-toggle="tab" href="#nav-home" role="tab" aria-controls="nav-home" aria-selected="true">AMT Survey</a>
        <a class="nav-item nav-link" id="nav-profile-tab" data-toggle="tab" href="#eval" role="tab" aria-controls="nav-profile" aria-selected="false">Evaluation Server</a>
      </div>
    </nav>

    <div class="tab-content" id="nav-tabContent"> -->

      <!-- The survey results -->
      <!-- <div class="tab-pane fade" id="nav-home" role="tabpanel" aria-labelledby="nav-home-tab">
        <div class='container'>
          <span class="border border-white">
          <p><b>Human study results</b> We submit a survey to the Amazon Mechanical Turk service in which humans are asked to vote on whether one set of detection mistakes are more dangerous than another set of detection mistakes. An example of the instructions given to the workers is shown below.
          </p>
          </span>
          <span class="border border-white">
            <img src='https://github.com/L6whTA/L6whTA.github.io/blob/master/amt.png?raw=true' class="img-fluid" height=240 style="border:5px solid #007bff; border-radius: 25px;">
            <br>
            <br>
          </span>

          <span class="border border-white">
          <p>
          We recorded 730 responses and found that human's agreed with PKL (our metric) over NDS in <b>79%</b> of scenarios. The 75 scenarios used in the survey are shown below. For each of the scenarios, the two detection sequences with different detection mistakes are shown on the left. The scene that PKL considers more dangerous is boxed in <font color="green">green</font>. The comments that the workers wrote about each of the scenes are shown to the right. The comment is colored <font color="green">green</font> if the commenter agreed with PKL and colored <font color="red">red</font> if the commenter disagreed with PKL.
          </p>
          </span>

          <h5 class='text-center'>Mechanical Turk Responses</h5>
          <div id="quizzes"></div>
        </div>
      </div> -->


      <!-- The planner results -->
      <!-- <div class="tab-pane fade show active" id="nav-profile" role="tabpanel" aria-labelledby="nav-profile-tab">
        <div class='container'>
          <span class="border border-white">
            <p><b>Distribution over trajectories</b> We train a model that predicts the future position of the ego vehicle for T timesteps. In the videos below, we visualize the sum of the heatmaps for all future timesteps as a single color for each of the vehicles in the scene. Different objects are given one of ten different colors to facilitate matching cars to their heatmaps. </p>
          </span>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/HtWLpcjD_yk" allowfullscreen></iframe>
          </div>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/BI7Gb39z8eY" allowfullscreen></iframe>
          </div>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/mVqqct7GsgY" allowfullscreen></iframe>
          </div>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/j5W2e_aWXXc" allowfullscreen></iframe>
          </div>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/azuUhN3-nuk" allowfullscreen></iframe>
          </div>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/F9FBll9shA8" allowfullscreen></iframe>
          </div>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/OyijrTKznMY" allowfullscreen></iframe>
          </div>
          <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/FmjvD3T6CbA" allowfullscreen></iframe>
          </div>
        </div>
      </div>

      <div class="tab-pane fade show" id="eval" role="tabpanel" aria-labelledby="nav-home-tab">
        <span class="border border-white">
        <p class="text-center">Coming Soon!</p>
        </span>
      </div>

      </div>

      <hr/> -->

      <!-- Add gifs -->
      <script>
      </script>



    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </body>
</html>