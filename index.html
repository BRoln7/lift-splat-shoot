<!doctype html>
<html lang="en">
  <head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156935549-3"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-156935549-3');
  </script>



    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Other -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/handlebars.js/4.4.2/handlebars.min.js"></script>

    <title>Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</title>
  </head>
  <body>

    <!-- header -->
    <div class='jumbotron' style="background-color:#e6e9ec">
    <div class="container">
    <h1 class="text-center">Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</h1>
    <p class='text-center'><a href="https://scholar.google.com/citations?user=VVIAoY0AAAAJ&hl=en" target="_blank">Jonah Philion</a>, <a href="http://www.cs.toronto.edu/~fidler/" target="_blank">Sanja Fidler</a></p>
    <p class='text-center'>NVIDIA, Vector Institute, University of Toronto</p>
    <p class='text-center'>ECCV 2020</p>
    <p class='text-center'><img src='imgs/nusc.gif' class='img-fluid' style='height:250px; border-radius:15px; padding:5px'></p>
    <!-- <iframe src="https://drive.google.com/file/d/1XwqzDYfzXhky1WNuXTKy7i-jVXgp4hc_/preview" width="640" height="480" autoplay></iframe> -->
    <!-- <div class="embed-responsive embed-responsive-16by9" style='height:250px; width:444px; margin:auto;'>
      <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/oL5ISk6BnDE?autoplay=1" allowfullscreen></iframe>
    </div> -->
    </div>
    </div>

    <div class="container">

      <p>
        The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these represen- tations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that di- rectly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eye- view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’s- eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eye- view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar.
      </p>

    <hr/>

    <span class="border border-white">
    <h4 class="text-center">News</h4>
    <ul>
      <li>[TBA] code release</li>
      <li>[TBA] paper released on arxiv</li>
      <li>[July 2020] camera-ready submitted [<a href="imgs/paper.pdf" target="_blank">paper</a>]</li>
    </ul>
    </span>

    <hr/>

    <span class="border border-white">
      <h4 class="text-center">Paper</h4>
      <div class='row'>
        <div class='col'>
          <img src='imgs/icon.jpg' class='img-fluid float-right' style='height:180px; border: solid; border-radius:30px;'>
        </div>
        <div class='col'>
          <p class="card-text">Jonah Philion, Sanja Fidler</p>
          <p class="card-text">Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</p>
          <p class="card-text">ECCV, 2020. (poster) (to appear)</p>
          <p class="card-text">[<a href="imgs/paper.pdf" target="_blank">preprint</a>] [bibtex]</p>
        </div>
      </div>
    </span>

    <hr/>
    <h4 class='text-center'>Main Idea</h4>
    <b>Lift, Splat, Shoot</b> Our goal is to design a model that takes as input multi-view image data from any camera rig and outputs a semantics in the reference frame of the camera rig as determined by the extrinsics and intrinsics of the cameras. The tasks we consider in this paper are bird's-eye-view vehicle segmentation, bird's-eye-view lane segmentation, drivable area segmentation, and motion planning.

    <p class='text-center'><img src='imgs/newsplat.jpg' class='img-fluid' style="border:0px solid #000000; border-radius: 15px; height: 120px;"></p>

    <b>Learning Cost Maps for Planning</b> We frame end-to-end motion planning as classification over a set of template trajectories (left). We define the logit for template to be the sum of values in the bird's-eye-view cost map output by our model (right). We then train the model to maximize the likelihood of expert trajectories.

    <p class='text-center'><img src='imgs/eq.png' class='img-fluid' style="border:0px solid #000000; border-radius: 15px; height: 120px;"></p>

    <b>Equivariance</b> To be maximally useful, models that perform inference in the bird's-eye-view frame need to generalize to any choice of bird's-eye-view coordinates. Our model is designed such that it roughly respects equivariance under translations and rotations of the camera extrinsics.

    <div class='row'>
      <div class='col-6 text-center'>
        <img src='imgs/sym.gif' class='img-fluid' style="border:1px solid #000000; border-radius: 25px;">
      </div>
      <div class='col-6 text-center'>
        <img src='imgs/rot.gif' class='img-fluid' style="border:1px solid #000000; border-radius: 25px;">
      </div>
    </div>
    <br>

    <b>Results</b> We outperform baselines on bird's-eye-view segmentation. We demonstrate transfer across camera rigs in two scenarios of increasing difficulty. In the first, we drop cameras at test time from the same camera rig that was used during training. In the second, we test on an entirely new camera rig (Lyft dataset) from what was used during training (nuScenes dataset).

    <p class='text-center'><img src='imgs/results.png' class='img-fluid' style="border:0px solid #000000; border-radius: 15px; height: 120px;"></p>

    <p class='text-center'><img src='imgs/nusc.gif' class='img-fluid' style='height:250px; border-radius:15px; padding:5px'></p>

    <div class='row'>
      <div class='col-6 text-center'>
        <iframe src="https://drive.google.com/file/d/1dGU0zmsxJgFtXMkkHrD2P6DB_JjlvYnQ/preview" width="440" height="280"></iframe>
      </div>
      <div class='col-6 text-center'>
        <iframe src="https://drive.google.com/file/d/1XwqzDYfzXhky1WNuXTKy7i-jVXgp4hc_/preview" width="440" height="280"></iframe>
      </div>
    </div>

    <hr/>
    <h4 class='text-center'>ECCV 2020 1 minute video</h4>
    <div class="embed-responsive embed-responsive-16by9">
      <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/ypQQUG4nFJY" style='display:block;' allowfullscreen></iframe>
    </div>
    <br>

    <hr/>
    <h4 class='text-center'>ECCV 2020 10 minute video</h4>
    <div class="embed-responsive embed-responsive-16by9">
      <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/oL5ISk6BnDE" allowfullscreen></iframe>
    </div>
    <br>



    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  </body>
</html>